{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Memory management utilities\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU and CPU memory caches\"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Memory cleared\")\n",
        "\n",
        "# Run this before each full execution\n",
        "clear_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wDQXx7cBlt0",
        "outputId": "614b1da3-175f-4925-da5a-fdad7bb02d8f"
      },
      "id": "5wDQXx7cBlt0",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "f981d046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f981d046",
        "outputId": "66f52511-cb5c-4fbb-c5cf-5c5416e6c9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PretrainedSED' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Installations & Setup (Run once per session)\n",
        "!pip install --quiet numpy pandas matplotlib scikit-learn torch torchvision torchaudio pytorch-lightning wandb rich ipywidgets tabulate tqdm\n",
        "!git clone https://github.com/fschmid56/PretrainedSED.git\n",
        "import sys\n",
        "sys.path.append('/content/PretrainedSED')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "5665ac41",
      "metadata": {
        "id": "5665ac41"
      },
      "outputs": [],
      "source": [
        "# Imports & Config (Run once per session)\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from PretrainedSED.models.atstframe.audio_transformer import FrameASTModel\n",
        "\n",
        "FP_costs = [1, 1, 2, 3, 3, 3, 3, 3, 3, 3]\n",
        "FN_costs = [5, 5, 5, 10, 20, 15, 20, 15, 25, 15]\n",
        "\n",
        "CONFIG = {\n",
        "    # Seed for reproducibility\n",
        "    \"seed\": 42,\n",
        "\n",
        "    # Data\n",
        "    \"batch_size\": 4,                   # Reduced to avoid OOM\n",
        "    \"accumulate_grad_batches\": 2,     # Effective batch size = 8\n",
        "    \"num_workers\": 4,\n",
        "    \"use_mel\": True,                 # Switch between spectrogram vs. embedding mode\n",
        "    \"pca_components\": 128,            # Required if use_mel=False\n",
        "\n",
        "    # Model\n",
        "    \"model_type\": \"atst\",\n",
        "    \"pretrained\": True,\n",
        "    \"dropout\": 0.3,\n",
        "    \"thresholds\": [0.2, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
        "    \"pos_weights\": torch.tensor([fn/fp for fn, fp in zip(FN_costs, FP_costs)], dtype=torch.float32),\n",
        "\n",
        "    # Optimization\n",
        "    \"lr\": 1.5e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"gradient_clip_val\": 0.,\n",
        "    \"precision\": \"bf16-mixed\",\n",
        "    \"enable_flash_attention\": True,\n",
        "\n",
        "    # Scheduler\n",
        "    \"scheduler\": \"cosine\",\n",
        "    \"warmup_epochs\": 5,\n",
        "\n",
        "    # Training\n",
        "    \"max_epochs\": 75,\n",
        "    \"patience\": 8,\n",
        "    \"limit_train_batches\": 0.5,       # Faster iteration (optional)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "4cf22d24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cf22d24",
        "outputId": "1072db56-78d8-470e-f619-c66bf74eac2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded dataset to: /content/mlpc2025_dataset/data\n",
            "TARGET_CLASSES: ['Speech', 'Shout', 'Chainsaw', 'Jackhammer', 'Lawn Mower', 'Power Drill', 'Dog Bark', 'Rooster Crow', 'Horn Honk', 'Siren']\n",
            "Functions available: [<function get_ground_truth_df at 0x7e110cbce340>, <function get_segment_prediction_df at 0x7e110cbce3e0>, <function check_dataframe at 0x7e110cbce160>, <function total_cost at 0x7e110cbce200>]\n"
          ]
        }
      ],
      "source": [
        "# Data Download (Run once per session - with retry logic)\n",
        "import time\n",
        "from huggingface_hub import hf_hub_download, HfFileSystem\n",
        "\n",
        "def download_with_retry(max_retries=3):\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            # Download compute_cost.py\n",
        "            if not os.path.exists(\"compute_cost.py\"):\n",
        "                print(\"Downloading compute_cost.py...\")\n",
        "                pyfile_path = hf_hub_download(\n",
        "                    repo_id=\"fschmid56/mlpc2025_dataset\",\n",
        "                    filename=\"compute_cost.py\",\n",
        "                    repo_type=\"dataset\"\n",
        "                )\n",
        "                shutil.copy(pyfile_path, \"compute_cost.py\")\n",
        "\n",
        "            # Download dataset\n",
        "            dataset_path = \"/content/mlpc2025_dataset/data\"\n",
        "            if not os.path.exists(dataset_path):\n",
        "                print(\"Downloading dataset...\")\n",
        "                os.makedirs(\"/content/mlpc2025_dataset\", exist_ok=True)\n",
        "\n",
        "                # Try direct download first\n",
        "                try:\n",
        "                    zip_path = hf_hub_download(\n",
        "                        repo_id=\"fschmid56/mlpc2025_dataset\",\n",
        "                        filename=\"mlpc2025_dataset.zip\",\n",
        "                        repo_type=\"dataset\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"HF Hub download failed, trying alternative method: {str(e)}\")\n",
        "                    # Alternative download using wget\n",
        "                    !wget https://huggingface.co/datasets/fschmid56/mlpc2025_dataset/resolve/main/mlpc2025_dataset.zip -O /content/mlpc2025_dataset.zip\n",
        "                    zip_path = \"/content/mlpc2025_dataset.zip\"\n",
        "\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(\"/content/mlpc2025_dataset\")\n",
        "\n",
        "                # Clean up\n",
        "                if os.path.exists(zip_path):\n",
        "                    os.remove(zip_path)\n",
        "\n",
        "            return dataset_path\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {i+1} failed: {str(e)}\")\n",
        "            if i < max_retries - 1:\n",
        "                print(\"Retrying in 10 seconds...\")\n",
        "                time.sleep(10)\n",
        "    raise Exception(\"Failed after multiple retries. You can try manually downloading from:\\n\"\n",
        "                   \"https://huggingface.co/datasets/fschmid56/mlpc2025_dataset/tree/main\\n\"\n",
        "                   \"and uploading to your Colab session.\")\n",
        "\n",
        "# Run download\n",
        "try:\n",
        "    DATASET_PATH = download_with_retry()\n",
        "    print(\"Successfully downloaded dataset to:\", DATASET_PATH)\n",
        "\n",
        "    # Import after setup\n",
        "    from compute_cost import CLASSES as TARGET_CLASSES, get_ground_truth_df, get_segment_prediction_df, check_dataframe, total_cost\n",
        "    print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n",
        "    print(\"Functions available:\", [get_ground_truth_df, get_segment_prediction_df, check_dataframe, total_cost])\n",
        "except Exception as e:\n",
        "    print(str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "099f0852",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099f0852",
        "outputId": "afabf2ab-36c5-40fb-84ac-22b1f6685bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached data...\n"
          ]
        }
      ],
      "source": [
        "def load_data_with_cache(dataset_path):\n",
        "    cache_file = os.path.join(dataset_path, \"data_cache.pkl\")\n",
        "\n",
        "    if os.path.exists(cache_file):\n",
        "        print(\"Loading cached data...\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    print(\"Processing data from scratch...\")\n",
        "    metadata = pd.read_csv(os.path.join(dataset_path, 'metadata.csv'))\n",
        "    all_files = metadata['filename'].unique()\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    train_files, temp_files = train_test_split(\n",
        "        all_files, test_size=0.4, random_state=CONFIG['seed'])\n",
        "    val_files, test_files = train_test_split(\n",
        "        temp_files, test_size=0.5, random_state=CONFIG['seed'])\n",
        "\n",
        "    # Load features\n",
        "    def load_features(files):\n",
        "        X = []\n",
        "        Y = {c: [] for c in TARGET_CLASSES}\n",
        "\n",
        "        for fname in files:\n",
        "            base = os.path.splitext(fname)[0]\n",
        "            feat_path = os.path.join(dataset_path, 'audio_features', base + '.npz')\n",
        "            features = np.load(feat_path)['embeddings']\n",
        "            X.append(features)\n",
        "\n",
        "            label_path = os.path.join(dataset_path, 'labels', base + '_labels.npz')\n",
        "            labels = np.load(label_path)\n",
        "            for c in TARGET_CLASSES:\n",
        "                Y[c].append((np.max(labels[c], axis=1) > 0).astype(int))\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = load_features(train_files)\n",
        "    X_val, Y_val = load_features(val_files)\n",
        "    X_test, Y_test = load_features(test_files)\n",
        "\n",
        "    # Apply PCA if using embeddings\n",
        "    if not CONFIG['use_mel']:\n",
        "        # Determine safe number of components\n",
        "        n_samples = X_train[0].shape[0]\n",
        "        n_features = X_train[0].shape[1]\n",
        "        safe_components = min(n_samples, n_features, CONFIG['pca_components'])\n",
        "        print(f\"Applying PCA with {safe_components} components (samples: {n_samples}, features: {n_features})\")\n",
        "\n",
        "        pca = PCA(safe_components)\n",
        "        X_train = [pca.fit_transform(x) if i == 0 else pca.transform(x)\n",
        "                  for i, x in enumerate(X_train)]\n",
        "        X_val = [pca.transform(x) for x in X_val]\n",
        "        X_test = [pca.transform(x) for x in X_test]\n",
        "\n",
        "    # Cache results\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump((X_train, Y_train, train_files,\n",
        "                    X_val, Y_val, val_files,\n",
        "                    X_test, Y_test, test_files), f)\n",
        "\n",
        "    return X_train, Y_train, train_files, X_val, Y_val, val_files, X_test, Y_test, test_files\n",
        "\n",
        "data = load_data_with_cache(DATASET_PATH)\n",
        "X_train, Y_train, train_files, X_val, Y_val, val_files, X_test, Y_test, test_files = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "0fc39a86",
      "metadata": {
        "id": "0fc39a86"
      },
      "outputs": [],
      "source": [
        "class ATSTFrameSED(nn.Module):\n",
        "    def __init__(self, num_classes=10, pretrained=True, use_mel=True, input_dim=128):\n",
        "        super().__init__()\n",
        "        self.use_mel = use_mel\n",
        "        self.target_frames = 512\n",
        "        self.feature_adapter = nn.Sequential(\n",
        "            nn.Linear(768, 1024),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(1024)\n",
        "        )\n",
        "\n",
        "        if self.use_mel:\n",
        "            # FrameAST model for spectrogram input\n",
        "            self.atst = FrameASTModel(\n",
        "                patch_h=8,\n",
        "                patch_w=4,\n",
        "                atst_dropout=0.1,\n",
        "                num_classes=num_classes,\n",
        "                pos_type=\"cut\",\n",
        "                nprompt=0\n",
        "            )\n",
        "        else:\n",
        "            # For embeddings: project input dim to transformer dim\n",
        "            self.input_proj = nn.Linear(input_dim, 768)\n",
        "\n",
        "        self.output_adapter = nn.Sequential(\n",
        "            nn.Linear(768, 768),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(768, num_classes)\n",
        "        )\n",
        "\n",
        "        if pretrained and self.use_mel:\n",
        "            self.load_pretrained_weights()\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        if lengths is None:\n",
        "            lengths = torch.full((x.size(0),), x.size(-1), device=x.device)\n",
        "\n",
        "        if self.use_mel:\n",
        "            # Input: [B, 1, 64, T]\n",
        "            dummy_mask = torch.zeros((x.size(0), x.size(-1)),\n",
        "                                     dtype=torch.bool, device=x.device)\n",
        "            x_token, _, _, _, _, _ = self.atst.prepare_tokens(\n",
        "                x, mask_index=dummy_mask, length=lengths, mask=False)\n",
        "\n",
        "            for blk in self.atst.blocks:\n",
        "                x_token = blk(x_token, lengths)\n",
        "\n",
        "            frame_repr = self.atst.norm_frame(x_token)[:, :self.target_frames, :]\n",
        "        else:\n",
        "            # Input: [B, T, D] → project to match transformer output\n",
        "            frame_repr = self.input_proj(x)\n",
        "\n",
        "        return self.output_adapter(frame_repr)\n",
        "\n",
        "    def load_pretrained_weights(self):\n",
        "        pretrained_url = \"https://github.com/fschmid56/PretrainedSED/releases/download/v0.0.1/ATST-F_strong_1.pt\"\n",
        "        try:\n",
        "            state_dict = torch.hub.load_state_dict_from_url(\n",
        "                pretrained_url, map_location='cpu'\n",
        "            )\n",
        "            if 'model' in state_dict:\n",
        "                state_dict = state_dict['model']\n",
        "            state_dict = {k: v for k, v in state_dict.items() if 'pos_embed' not in k}\n",
        "            self.atst.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Loaded pretrained weights (excluding positional embeddings)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load weights: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "ffe1fd86",
      "metadata": {
        "id": "ffe1fd86"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, Y, classes, filenames, audio_dir, is_training=False):\n",
        "        self.Y = Y\n",
        "        self.classes = classes\n",
        "        self.filenames = filenames\n",
        "        self.audio_dir = audio_dir\n",
        "        self.is_training = is_training  # Training mode flag\n",
        "\n",
        "        # Spectrogram transform\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=16000,\n",
        "            n_fft=512,\n",
        "            hop_length=160,\n",
        "            n_mels=64,\n",
        "            f_min=50,\n",
        "            f_max=7500\n",
        "        )\n",
        "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "        # Initialize augmentation transforms\n",
        "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=100)  # ~1.0s\n",
        "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=8)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load audio\n",
        "        audio_path = os.path.join(self.audio_dir, self.filenames[idx])\n",
        "        try:\n",
        "            x, sr = torchaudio.load(audio_path)\n",
        "            assert sr == 16000, f\"Unexpected sample rate {sr}\"\n",
        "            x = x.squeeze(0)  # [1, T] -> [T]\n",
        "\n",
        "            # Apply random gain augmentation during training (-5dB to +5dB)\n",
        "            if self.is_training:\n",
        "                gain = torch.empty(1).uniform_(-5, 5)  # More efficient than rand()\n",
        "                x = x * (10 ** (gain / 20))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {audio_path}: {str(e)}\")\n",
        "            # Return silent audio as fallback\n",
        "            x = torch.zeros(16000)  # 1s of silence\n",
        "\n",
        "        # Compute spectrogram\n",
        "        with torch.no_grad():\n",
        "            spec = self.mel_transform(x)  # [64, T']\n",
        "\n",
        "            # Apply SpecAugment during training\n",
        "            if self.is_training:\n",
        "                spec = self.time_mask(self.freq_mask(spec))\n",
        "\n",
        "            # Pad/trim to fixed 512 frames (~5.12s)\n",
        "            spec = spec[..., :512] if spec.size(-1) > 512 else F.pad(spec, (0, 512 - spec.size(-1)))\n",
        "\n",
        "            # Normalize dB-scale to [0,1]\n",
        "            spec = self.amp_to_db(spec)\n",
        "            spec = (spec + 80) / 80  # Assuming -80dB floor\n",
        "            spec = spec.unsqueeze(0)  # [1, 64, 512]\n",
        "\n",
        "        # Process labels\n",
        "        y = torch.stack([\n",
        "            torch.tensor(self.Y[c][idx], dtype=torch.float32)\n",
        "            for c in self.classes\n",
        "        ], dim=1)  # [T, 10]\n",
        "\n",
        "        # Align labels with spectrogram length\n",
        "        y = y[:512] if y.size(0) >= 512 else F.pad(y, (0, 0, 0, 512 - y.size(0)))\n",
        "\n",
        "        return spec, y, torch.tensor(spec.shape[-1]), self.filenames[idx]\n",
        "\n",
        "class SEDDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dataset_path, batch_size=16, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.dataset_path = dataset_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.audio_dir = os.path.join(dataset_path, 'audio')\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        data = load_data_with_cache(self.dataset_path)\n",
        "        _, Y_train, train_files, _, Y_val, val_files, _, Y_test, test_files = data\n",
        "\n",
        "        self.train_ds = SequenceDataset(Y_train, TARGET_CLASSES, train_files,\n",
        "                                     self.audio_dir, is_training=True)  # Enable augmentations\n",
        "        self.val_ds = SequenceDataset(Y_val, TARGET_CLASSES, val_files, self.audio_dir)\n",
        "        self.test_ds = SequenceDataset(Y_test, TARGET_CLASSES, test_files, self.audio_dir)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # Each spec is [1, 64, 624] → stack becomes [B, 1, 64, 624]\n",
        "        specs = torch.stack([item[0] for item in batch])      # [B, 1, 64, 624]\n",
        "        labels = torch.stack([item[1] for item in batch])     # [B, 624, 10]\n",
        "        lengths = torch.stack([item[2] for item in batch])    # [B]\n",
        "        filenames = [item[3] for item in batch]               # List[str]\n",
        "        return specs, labels, lengths, filenames\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.collate_fn,\n",
        "            persistent_workers=True\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "2f5dacfe",
      "metadata": {
        "id": "2f5dacfe"
      },
      "outputs": [],
      "source": [
        "class SEDLightningModule(pl.LightningModule):\n",
        "    def __init__(self, classes, lr=1e-4, thresholds=None, pos_weight=None):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = ATSTFrameSED(\n",
        "            num_classes=len(classes),\n",
        "            pretrained=True,\n",
        "            use_mel=CONFIG['use_mel'],\n",
        "            input_dim=CONFIG.get('pca_components', 128)\n",
        "        )\n",
        "\n",
        "        # Convert thresholds to tensor if provided\n",
        "        if thresholds is not None:\n",
        "            self.register_buffer('thresholds', torch.tensor(thresholds, dtype=torch.float32))\n",
        "        else:\n",
        "            self.thresholds = None\n",
        "\n",
        "        self.criterion = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=pos_weight,\n",
        "            reduction='none'\n",
        "        )\n",
        "        self.criterion = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=pos_weight,\n",
        "            reduction='none'\n",
        "        )\n",
        "        self.val_preds = {c: [] for c in classes}\n",
        "        self.val_targets = {c: [] for c in classes}\n",
        "        self.val_filenames = []\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        return self.model(x, lengths)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        if not isinstance(batch, (list, tuple)):\n",
        "            raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(\"[DEBUG] Entered first training_step of epoch\")\n",
        "            print(f\"[DEBUG] Batch type: {type(batch)}, length: {len(batch)}\")\n",
        "            for i, item in enumerate(batch):\n",
        "                print(f\"[DEBUG] Item {i}: type={type(item)}, shape={getattr(item, 'shape', 'list' if isinstance(item, list) else 'unknown')}\")\n",
        "\n",
        "        # Safely unpack or fail with more info\n",
        "        try:\n",
        "            X, Y, lengths, _ = batch\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"[ERROR] Batch unpacking failed (expected 4 items). Batch content: {batch}\") from e\n",
        "\n",
        "        logits = self(X, lengths)\n",
        "        mask = (torch.arange(logits.size(1), device=logits.device)[None, :] < lengths[:, None]).unsqueeze(-1).float()\n",
        "        loss = (self.criterion(logits, Y.float()) * mask).sum() / mask.sum()\n",
        "        self.log(\"train/loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        X, Y, lengths, filenames = batch  # Matches dataset format\n",
        "        logits = self(X, lengths)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        if self.thresholds is not None:\n",
        "            # Expand thresholds to match logits shape [B,T,C]\n",
        "            thresholds = self.thresholds.view(1, 1, -1).expand_as(probs)\n",
        "            preds = (probs > thresholds).long()\n",
        "        else:\n",
        "            # Fallback to default 0.5 threshold\n",
        "            preds = (probs > 0.5).long()\n",
        "\n",
        "        for i, cls in enumerate(self.hparams.classes):\n",
        "            self.val_preds[cls].append(preds[..., i].cpu())\n",
        "            self.val_targets[cls].append(Y[..., i].long().cpu())\n",
        "\n",
        "        self.val_filenames.extend(filenames)\n",
        "        mask = (torch.arange(logits.size(1), device=logits.device))[None, :] < lengths[:, None]\n",
        "        mask = mask.unsqueeze(-1).float()\n",
        "        loss = (self.criterion(logits, Y.float()) * mask).sum() / mask.sum()\n",
        "        self.log(\"val/loss\", loss, prog_bar=True, sync_dist=True)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        preds_numpy = {\n",
        "            cls: torch.cat(self.val_preds[cls]).numpy()\n",
        "            for cls in self.hparams.classes\n",
        "        }\n",
        "        targets_numpy = {\n",
        "            cls: torch.cat(self.val_targets[cls]).numpy()\n",
        "            for cls in self.hparams.classes\n",
        "        }\n",
        "\n",
        "        total_cost, _ = self.evaluate_cost(\n",
        "            preds_numpy=preds_numpy,\n",
        "            targets_numpy=targets_numpy,\n",
        "            filenames=list(set(self.val_filenames))\n",
        "        )\n",
        "        self.log(\"val/cost\", total_cost, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        self.val_preds = {c: [] for c in self.hparams.classes}\n",
        "        self.val_targets = {c: [] for c in self.hparams.classes}\n",
        "        self.val_filenames = []\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.lr,\n",
        "            weight_decay=CONFIG['weight_decay'],\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=CONFIG['warmup_epochs'] * len(self.trainer.datamodule.train_dataloader()),\n",
        "            T_mult=1,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def evaluate_cost(self, preds_numpy, targets_numpy, filenames):\n",
        "        predictions_dict = {\n",
        "            filename: {cls: preds_numpy[cls][i] for cls in self.hparams.classes}\n",
        "            for i, filename in enumerate(filenames)\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            gt_df = get_ground_truth_df(filenames, DATASET_PATH)\n",
        "            pred_df = get_segment_prediction_df(predictions_dict, self.hparams.classes)\n",
        "\n",
        "            merged = pred_df.merge(\n",
        "                gt_df,\n",
        "                on=[\"filename\", \"onset\"],\n",
        "                how=\"inner\",\n",
        "                suffixes=(\"_pred\", \"_true\")\n",
        "            )\n",
        "            if len(merged) == 0:\n",
        "                raise ValueError(\"No matching rows between predictions and ground truth\")\n",
        "\n",
        "            cost_value, breakdown = total_cost(\n",
        "                merged[[col for col in merged.columns if col.endswith(\"_pred\") or col in [\"filename\", \"onset\"]]].rename(\n",
        "                    columns={col: col.replace(\"_pred\", \"\") for col in merged.columns if col.endswith(\"_pred\")}\n",
        "                ),\n",
        "                merged[[col for col in merged.columns if col.endswith(\"_true\") or col in [\"filename\", \"onset\"]]].rename(\n",
        "                    columns={col: col.replace(\"_true\", \"\") for col in merged.columns if col.endswith(\"_true\")}\n",
        "                )\n",
        "            )\n",
        "            return cost_value, breakdown\n",
        "        except Exception as e:\n",
        "            print(f\"Error during cost calculation: {str(e)}\")\n",
        "            return float('inf'), {}\n",
        "\n",
        "    test_step = validation_step\n",
        "    on_test_epoch_end = on_validation_epoch_end"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_data_pipeline(dm):\n",
        "    print(\"\\n=== VERIFYING DATA PIPELINE ===\")\n",
        "\n",
        "    # Check dataset\n",
        "    sample = dm.train_ds[0]\n",
        "    print(f\"Single sample contains {len(sample)} items:\")\n",
        "    for i, item in enumerate(sample):\n",
        "        print(f\"  Item {i}: Type={type(item)}, Shape={item.shape if hasattr(item, 'shape') else 'str'}\")\n",
        "\n",
        "    # Check batch\n",
        "    batch = next(iter(dm.train_dataloader()))\n",
        "    print(f\"\\nBatch contains {len(batch)} items:\")\n",
        "    for i, item in enumerate(batch):\n",
        "        if isinstance(item, (list, tuple)):\n",
        "            print(f\"  Item {i}: Type={type(item)}, Length={len(item)}\")\n",
        "        else:\n",
        "            print(f\"  Item {i}: Type={type(item)}, Shape={item.shape if hasattr(item, 'shape') else 'N/A'}\")\n",
        "\n",
        "    # Verify model can process the batch\n",
        "    model = SEDLightningModule(TARGET_CLASSES, lr=CONFIG['lr'])\n",
        "    try:\n",
        "        X, Y, lengths, _ = batch\n",
        "        outputs = model(X.to('cuda'), lengths.to('cuda'))\n",
        "        print(f\"\\nModel output shape: {outputs.shape} (should be [16, 624, 10])\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel forward pass failed: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "QSw-jLtVYB-B"
      },
      "id": "QSw-jLtVYB-B",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "ce51e612",
      "metadata": {
        "id": "ce51e612"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "def train_model():\n",
        "    # Memory Management\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"GPU Memory cleared: {torch.cuda.memory_allocated()/1e9:.1f}GB free\")\n",
        "\n",
        "    # Data Setup\n",
        "    dm = SEDDataModule(\n",
        "        dataset_path=DATASET_PATH,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        num_workers=CONFIG['num_workers']\n",
        "    )\n",
        "    dm.setup()\n",
        "    print(\"Data module ready\")\n",
        "\n",
        "    # Model Initialization\n",
        "    model = SEDLightningModule(\n",
        "        classes=TARGET_CLASSES,\n",
        "        lr=CONFIG['lr'],\n",
        "        thresholds=CONFIG['thresholds'],\n",
        "        pos_weight=CONFIG['pos_weights']  # Now properly a tensor\n",
        "    )\n",
        "    print(f\"Model initialized with lr={CONFIG['lr']}\")\n",
        "\n",
        "    # Setup Weights & Biases logger\n",
        "    run_type = \"mel\" if CONFIG['use_mel'] else \"embed\"\n",
        "    wandb_logger = WandbLogger(project=\"atst_sed\", name=f\"run_{run_type}\", log_model=True)\n",
        "    wandb_logger.log_hyperparams(CONFIG)\n",
        "\n",
        "    # Trainer Configuration\n",
        "    trainer = pl.Trainer(\n",
        "        logger=wandb_logger,\n",
        "        accelerator=\"auto\",\n",
        "        devices=\"auto\",\n",
        "        max_epochs=CONFIG['max_epochs'],\n",
        "        precision=CONFIG['precision'],\n",
        "        accumulate_grad_batches=CONFIG['accumulate_grad_batches'],\n",
        "        gradient_clip_val=CONFIG['gradient_clip_val'],\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(\n",
        "                monitor=\"val/cost\",\n",
        "                mode=\"min\",\n",
        "                save_top_k=1,\n",
        "                filename=\"best-{epoch:02d}-{val_cost:.2f}\",\n",
        "                auto_insert_metric_name=False\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor=\"val/cost\",\n",
        "                patience=CONFIG['patience'],\n",
        "                mode=\"min\",\n",
        "                min_delta=0.1,\n",
        "                verbose=True\n",
        "            ),\n",
        "            LearningRateMonitor(),\n",
        "            RichProgressBar(refresh_rate=5)\n",
        "        ],\n",
        "        log_every_n_steps=1,\n",
        "        check_val_every_n_epoch=3,         # <-- validate every 3 epochs (faster)\n",
        "        limit_train_batches=0.5,           # <-- use only 50% of data per epoch (for tuning)\n",
        "        enable_model_summary=True\n",
        "    )\n",
        "\n",
        "    # Training Execution\n",
        "    print(\"\\nStarting Training...\")\n",
        "    print(f\"Batch Size: {CONFIG['batch_size']} (Effective: {CONFIG['batch_size']*CONFIG['accumulate_grad_batches']})\")\n",
        "    print(f\"Steps per Epoch: {len(dm.train_dataloader())}\")\n",
        "\n",
        "    # === RUN without suppressing errors ===\n",
        "    trainer.fit(model, dm)\n",
        "    test_results = trainer.test(model, dm, ckpt_path=\"best\")\n",
        "    print(f\"\\nFinal Validation Cost: {test_results[0]['val/cost']:.2f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e979a45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775,
          "referenced_widgets": [
            "8198d7d0207c49c9b903e8009cdbc3ee",
            "ee4870d495a14b7388d6c072f3cb06b1"
          ]
        },
        "collapsed": true,
        "id": "7e979a45",
        "outputId": "7ca6d9c6-e54d-4f39-bcfd-300aaa8f8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flash Attention Enabled (before): True\n",
            "Flash Attention Enabled (after): True\n",
            "GPU Memory cleared: 0.0GB free\n",
            "Loading cached data...\n",
            "Data module ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights (excluding positional embeddings)\n",
            "Model initialized with lr=0.00015\n",
            "\n",
            "Starting Training...\n",
            "Batch Size: 4 (Effective: 8)\n",
            "Steps per Epoch: 1235\n",
            "Loading cached data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model     │ ATSTFrameSED      │ 88.0 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ criterion │ BCEWithLogitsLoss │      0 │ train │\n",
              "└───┴───────────┴───────────────────┴────────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model     │ ATSTFrameSED      │ 88.0 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ criterion │ BCEWithLogitsLoss │      0 │ train │\n",
              "└───┴───────────┴───────────────────┴────────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 88.0 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 88.0 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 351                                                                        \n",
              "\u001b[1mModules in train mode\u001b[0m: 185                                                                                         \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 88.0 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 88.0 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 351                                                                        \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 185                                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8198d7d0207c49c9b903e8009cdbc3ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Entered first training_step of epoch\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Entered first training_step of epoch\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Batch type: <class 'tuple'>, length: 4\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Batch type: &lt;class 'tuple'&gt;, length: 4\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 0: type=<class 'torch.Tensor'>, shape=torch.Size([4, 1, 64, 512])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 0: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4, 1, 64, 512])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 1: type=<class 'torch.Tensor'>, shape=torch.Size([4, 512, 10])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 1: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4, 512, 10])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 2: type=<class 'torch.Tensor'>, shape=torch.Size([4])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 2: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 3: type=<class 'list'>, shape=list\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 3: type=&lt;class 'list'&gt;, shape=list\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Entered first training_step of epoch\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Entered first training_step of epoch\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Batch type: <class 'tuple'>, length: 4\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Batch type: &lt;class 'tuple'&gt;, length: 4\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 0: type=<class 'torch.Tensor'>, shape=torch.Size([4, 1, 64, 512])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 0: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4, 1, 64, 512])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 1: type=<class 'torch.Tensor'>, shape=torch.Size([4, 512, 10])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 1: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4, 512, 10])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 2: type=<class 'torch.Tensor'>, shape=torch.Size([4])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 2: type=&lt;class 'torch.Tensor'&gt;, shape=torch.Size([4])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DEBUG] Item 3: type=<class 'list'>, shape=list\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DEBUG] Item 3: type=&lt;class 'list'&gt;, shape=list\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Execution Block (Run to train)\n",
        "if __name__ == \"__main__\":\n",
        "    import torch.backends.cuda\n",
        "\n",
        "    print(\"Flash Attention Enabled (before):\", torch.backends.cuda.flash_sdp_enabled())\n",
        "\n",
        "    # Force-enable Flash Attention modes\n",
        "    torch.backends.cuda.enable_flash_sdp(True)\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "    torch.backends.cuda.enable_math_sdp(False)\n",
        "\n",
        "    print(\"Flash Attention Enabled (after):\", torch.backends.cuda.flash_sdp_enabled())\n",
        "\n",
        "    # Optional: set matmul precision for speed\n",
        "    torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "    # Launch training\n",
        "    best_model = train_model()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8198d7d0207c49c9b903e8009cdbc3ee": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ee4870d495a14b7388d6c072f3cb06b1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 1/74 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m 475/617 \u001b[2m0:09:43 • 0:02:54\u001b[0m \u001b[2;4m0.82it/s\u001b[0m \u001b[3mv_num: hoe6 train/loss: 2.011\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 1/74 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━</span> 475/617 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:09:43 • 0:02:54</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.82it/s</span> <span style=\"font-style: italic\">v_num: hoe6 train/loss: 2.011</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "ee4870d495a14b7388d6c072f3cb06b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}