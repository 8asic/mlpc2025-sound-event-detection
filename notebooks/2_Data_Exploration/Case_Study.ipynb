{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8414ccd9",
   "metadata": {},
   "source": [
    "<h3>Case study</h3>\n",
    "Find two interesting recordings with at least two annotators and multiple annotations. Compare the temporal and textual annotations, and try to answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af141591",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import from our package\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, DatasetType\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_annotations, load_metadata\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# INITIAL SETUP (NEW VERSION)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "# Import from our package\n",
    "from src.config import config, DatasetType\n",
    "from src.data.loaders import load_annotations, load_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae45c82",
   "metadata": {},
   "source": [
    "\n",
    "<h5>1. Data Loading</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad08473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading data with automatic path resolution\n",
    "try:\n",
    "    # Load data for Task 2 (Exploration)\n",
    "    annotations_df, annotations_emb = load_annotations()\n",
    "    metadata_df, _, _ = load_metadata()\n",
    "    \n",
    "    # Calculate durations\n",
    "    annotations_df['duration'] = annotations_df['offset'] - annotations_df['onset']\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Annotations: {len(annotations_df)} records\")\n",
    "    print(f\"Metadata: {len(metadata_df)} records\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(\"ERROR: Could not load dataset\")\n",
    "    print(\"\\nSOLUTION: Please either:\")\n",
    "    print(f\"1. Place the dataset in: {config.get_path(DatasetType.EXPLORATION)}\")\n",
    "    print(\"2. Or set custom path using:\")\n",
    "    print(\"   config.set_path(DatasetType.EXPLORATION, Path('your/path'))\")\n",
    "    print(\"3. Or set environment variable:\")\n",
    "    print(\"   export MLPC_EXPLORATION_PATH='your/path'\")\n",
    "    print(\"\\nDownload from: https://cloud.cp.jku.at/index.php/s/YKJqiWjnQQAjiH5\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57721a94",
   "metadata": {},
   "source": [
    "<h5>(a) Identify similarities/differences between annotators</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3607cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for 185070.mp3\n",
    "print(\"Analysis for 185070.mp3:\")\n",
    "display(annotations_df.loc[annotations_df['filename'] == '185070.mp3'])\n",
    "\n",
    "# Analysis for 637068.mp3 \n",
    "print(\"\\nAnalysis for 637068.mp3:\")\n",
    "display(annotations_df.loc[annotations_df['filename'] == '637068.mp3'].sort_values(by=['annotator','onset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1cc35",
   "metadata": {},
   "source": [
    "<h5>(b) Compare annotations with metadata</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b088e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 185070.mp3 metadata\n",
    "print(\"Metadata for 185070.mp3:\")\n",
    "display(metadata_df.loc[metadata_df['filename'] == '185070.mp3'])\n",
    "\n",
    "# 637068.mp3 metadata\n",
    "print(\"\\nMetadata for 637068.mp3:\")\n",
    "display(metadata_df.loc[metadata_df['filename'] == '637068.mp3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a913310",
   "metadata": {},
   "source": [
    "<h5>(c) Verify annotation guidelines compliance</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ec427",
   "metadata": {},
   "source": [
    "<h6>Audio file: 185070.mp3</h6>\n",
    "Both annotations were done according to the task desctiption, however we can observe that there is a gap between 6's and 9's seconds, which correspornds to the onset and offset respectively, indicates that there is change over time which wasn't mentioned in both annotations. \n",
    "\n",
    "<h6>Audio file: 637068.mp3</h6>The students who decribed chosen regions by one-two words doesn't fullfil criteria of the textual annotation such as: descriptor, temporal and context, which makes annotations not clear and ambiguous, and the student who chose more wide region to describe several sounds at the same time  vialoate the criterea for temporal annotation, for some regions sound of \"cymbal\" appeares multiple times with difference more than one second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c5afc",
   "metadata": {},
   "source": [
    "<h4>2. Quantitative Analysis</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate annotation overlaps (identical to original)\n",
    "overlap_per_file_with_2_annotators = {}\n",
    "overlap_per_file_with_3_annotators = {}\n",
    "count_per_zero_overlap = 0\n",
    "unique_annontator_per_file = []\n",
    "total_number_files_with_2_annotators = 0\n",
    "total_number_files_with_3_annotators = 0\n",
    "precision_threshold = 0.1 # seconds\n",
    "\n",
    "for file, group in annotations_df.groupby('filename'):\n",
    "    counts = 0\n",
    "    counter_for_2_annotators = 0\n",
    "    counter_for_3_annotators = 0\n",
    "    \n",
    "    if group['annotator'].nunique() == 2:\n",
    "        total_number_files_with_2_annotators += 1\n",
    "        for (row1,annotator_1), (row2,annotator_2) in combinations(group.iterrows(),2):\n",
    "            if (annotator_1['annotator'] != annotator_2['annotator']):\n",
    "                if max(annotator_1['onset'], annotator_2['onset']) < min(annotator_1['offset'], annotator_2['offset']):\n",
    "                    if abs(annotator_1['duration'] - annotator_2['duration']) <= precision_threshold: # precision threshold of 0.9 seconds\n",
    "                        counter_for_2_annotators += 1\n",
    "    elif group['annotator'].nunique() == 3:\n",
    "        total_number_files_with_3_annotators += 1\n",
    "        for (row1,annotator_1), (row2,annotator_2),(row3,annotator_3) in combinations(group.iterrows(),3):\n",
    "            if (annotator_1['annotator'] != annotator_2['annotator']) and (annotator_1['annotator']!= annotator_3['annotator']) and (annotator_2['annotator']!= annotator_3['annotator']):\n",
    "                if max(annotator_1['onset'], annotator_2['onset'], annotator_3['onset']) < min(annotator_1['offset'], annotator_2['offset'],annotator_3['offset']):\n",
    "                    if abs(annotator_1['duration'] - annotator_2['duration']) <= precision_threshold and abs(annotator_1['duration'] - annotator_3['duration']) <= precision_threshold and abs(annotator_2['duration'] - annotator_3['duration']) <= precision_threshold:\n",
    "                        counter_for_3_annotators += 1\n",
    "    else:\n",
    "        counts = 0\n",
    "    overlap_per_file_with_2_annotators[file] = counter_for_2_annotators\n",
    "    overlap_per_file_with_3_annotators[file] = counter_for_3_annotators\n",
    "    \n",
    "    \n",
    "    if counts == 0:\n",
    "        count_per_zero_overlap += 1\n",
    "    unique_annontator_per_file.append(group['annotator'].nunique())\n",
    "\n",
    "num_of_files_with_overlaps_2_annotators = sum(1 for val in overlap_per_file_with_2_annotators.values() if val > 0)\n",
    "num_of_files_with_overlaps_3_annotators = sum(1 for val in overlap_per_file_with_3_annotators.values() if val > 0)\n",
    "\n",
    "file_max_overlap_2_annotators, num_overlap_max_2_annotators = max(overlap_per_file_with_2_annotators.items(), key=lambda x: x[1])\n",
    "file_max_overlap_3_annotators, num_overlap_max_3_annotators = max(overlap_per_file_with_3_annotators.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print results (unchanged)\n",
    "print(f'Total files with 2 annotators: {total_number_files_with_2_annotators}')\n",
    "print(f'Max overlapping regions: {num_overlap_max_2_annotators} (file: {file_max_overlap_2_annotators})')\n",
    "print(f'Files with â‰¥1 overlap (2 annotators): {num_of_files_with_overlaps_2_annotators}')\n",
    "\n",
    "print(f'\\nTotal files with 3 annotators: {total_number_files_with_3_annotators}')\n",
    "print(f'Max overlapping regions: {num_overlap_max_3_annotators} (file: {file_max_overlap_3_annotators})')\n",
    "print(f'Files with â‰¥1 overlap (3 annotators): {num_of_files_with_overlaps_3_annotators}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe0757",
   "metadata": {},
   "source": [
    "<h5>(b) Text annotation similarity</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text similarities (identical to original)\n",
    "similarities_2_annotators = []\n",
    "similarities_3_annotators = []\n",
    "\n",
    "for file, group in annotations_df.groupby('filename'):\n",
    "    if group['annotator'].nunique() == 2:\n",
    "        for (row1,annotator_1), (row2,annotator_2) in combinations(group.iterrows(),2):\n",
    "            if (annotator_1['annotator'] != annotator_2['annotator']):\n",
    "                if max(annotator_1['onset'], annotator_2['onset']) < min(annotator_1['offset'], annotator_2['offset']):\n",
    "                    if abs(annotator_1['duration'] - annotator_2['duration']) <= 0.9:\n",
    "                        em1 = annotations_emb[row1].reshape(1, -1)\n",
    "                        em2 = annotations_emb[row2].reshape(1,-1)\n",
    "                        sim = cosine_similarity(em1, em2)[0][0]\n",
    "                        similarities_2_annotators.append(sim)\n",
    "\n",
    "    elif group['annotator'].nunique() == 3:\n",
    "        for (row1,annotator_1), (row2,annotator_2),(row3,annotator_3) in combinations(group.iterrows(),3):\n",
    "            if (annotator_1['annotator'] != annotator_2['annotator']) and (annotator_1['annotator']!= annotator_3['annotator']) and (annotator_2['annotator']!= annotator_3['annotator']):\n",
    "                if max(annotator_1['onset'], annotator_2['onset'], annotator_3['onset']) < min(annotator_1['offset'], annotator_2['offset'],annotator_3['offset']):\n",
    "                    if abs(annotator_1['duration'] - annotator_2['duration']) <= 0.9 and abs(annotator_1['duration'] - annotator_3['duration']) <= 0.9 and abs(annotator_2['duration'] - annotator_3['duration']) <= 0.9:\n",
    "                        em1 = annotations_emb[row1].reshape(1, -1)\n",
    "                        em2 = annotations_emb[row2].reshape(1,-1)\n",
    "                        em3 = annotations_emb[row3].reshape(1,-1)\n",
    "                        sim1 = cosine_similarity(em1, em2)[0][0]\n",
    "                        sim2 = cosine_similarity(em1, em3)[0][0]\n",
    "                        sim3 = cosine_similarity(em2, em3)[0][0]\n",
    "                        avg = (sim1+sim2+sim3) / 3\n",
    "                        similarities_3_annotators.append(avg)\n",
    "\n",
    "print(f'\\nText similarity results:')\n",
    "print(f'Pairs compared: {len(similarities_2_annotators)}')\n",
    "print(f'Mean similarity (2 annotators): {np.mean(similarities_2_annotators):.3f}')\n",
    "print(f'Mean similarity (3 annotators): {np.mean(similarities_3_annotators):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
