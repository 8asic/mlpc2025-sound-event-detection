{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a987dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# IMPORTS AND SETUP\n",
    "# ======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59984a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# DATA CONFIGURATION\n",
    "# ======================\n",
    "categories = ['Airplane', 'Alarm', 'Beep/Bleep', 'Bell', 'Bicycle', 'Bird Chirp', 'Bus', 'Car', 'Cat Meow',\n",
    "        'Chainsaw', 'Clapping', 'Cough', 'Cow Moo', 'Cowbell', 'Crying', 'Dog Bark', 'Doorbell', 'Drip',\n",
    "        'Drums', 'Fire', 'Footsteps', 'Guitar', 'Hammer', 'Helicopter', 'Hiccup', 'Horn Honk', 'Horse Neigh',\n",
    "        'Insect Buzz', 'Jackhammer', 'Laughter', 'Lawn Mower', 'Motorcycle', 'Piano', 'Pig Oink', 'Power Drill',\n",
    "        'Power Saw', 'Rain', 'Rooster Crow', 'Saxophone', 'Sewing Machine', 'Sheep/Goat Bleat', 'Ship/Boat',\n",
    "        'Shout', 'Singing', 'Siren', 'Sneeze', 'Snoring', 'Speech', 'Stream/River', 'Thunder', 'Train', 'Truck',\n",
    "        'Trumpet', 'Vacuum Cleaner', 'Violin', 'Washing Machine', 'Waves', 'Wind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbd551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata and annotations...\n",
      "Data loading completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# DATA LOADING\n",
    "# ======================\n",
    "try:\n",
    "    print(\"Loading metadata and annotations...\")\n",
    "    ann_df = pd.read_csv('annotations.csv')\n",
    "    meta_df = pd.read_csv('metadata.csv')\n",
    "    \n",
    "    # Split filenames\n",
    "    train_filename = meta_df.sample(len(meta_df), random_state=42)['filename'].unique()[:int(len(meta_df)*0.7)]\n",
    "    validation_filename = meta_df.sample(len(meta_df), random_state=42)['filename'].unique()[int(len(meta_df)*0.7):int(len(meta_df)*0.9)]\n",
    "    test_filename = meta_df.sample(len(meta_df), random_state=42)['filename'].unique()[int(len(meta_df)*0.9):len(meta_df)]\n",
    "    \n",
    "    print(\"Data loading completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18468933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# HELPER FUNCTIONS\n",
    "# ======================\n",
    "def aggregate_labels(file_labels):\n",
    "    __y = []\n",
    "    for frame_labels in file_labels:\n",
    "        if sum(frame_labels) == 0:\n",
    "            __y.append([0])\n",
    "        elif np.count_nonzero(frame_labels) == len(frame_labels):\n",
    "             __y.append([1])\n",
    "        else:\n",
    "            __y.append([np.random.choice(frame_labels)])\n",
    "    return __y\n",
    "\n",
    "def read_files(file_names, features_dir='audio_features', labels_dir='labels'):\n",
    "    X = []\n",
    "    Y = {c: [] for c in categories}\n",
    "    \n",
    "    for f in file_names:\n",
    "        try:\n",
    "            # Load features\n",
    "            features_mel = np.load(os.path.join(features_dir, f.split('.')[0] + '.npz'))[\"melspectrogram\"]\n",
    "            features_mfcc = np.load(os.path.join(features_dir, f.split('.')[0] + '.npz'))[\"mfcc\"]\n",
    "            \n",
    "            # Process features\n",
    "            min_length = min(features_mel.shape[0], features_mfcc.shape[0])\n",
    "            features = np.concatenate([features_mel[:min_length], features_mfcc[:min_length]], axis=1)\n",
    "            X.append(features)\n",
    "            \n",
    "            # Process labels\n",
    "            y = np.load(os.path.join(labels_dir, f.split('.')[0] + '_labels.npz'))\n",
    "            for c in categories:\n",
    "                _y = aggregate_labels(y[c])\n",
    "                Y[c].extend(list(itertools.chain.from_iterable(_y))[:min_length])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {f}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.concatenate(X), np.array([Y[cls] for cls in categories]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73c3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocessing data...\n",
      "Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# DATA PREPROCESSING\n",
    "# ======================\n",
    "try:\n",
    "    print(\"\\nLoading and preprocessing data...\")\n",
    "    train_x, train_y = read_files(train_filename)\n",
    "    val_x, val_y = read_files(validation_filename)\n",
    "    test_x, test_y = read_files(test_filename)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train_x = scaler.fit_transform(train_x)\n",
    "    scaled_val_x = scaler.transform(val_x)\n",
    "    scaled_test_x = scaler.transform(test_x)\n",
    "    \n",
    "    print(\"Data preprocessing completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during preprocessing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92dfceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running FAISS similarity search...\n",
      "\n",
      "FAISS Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        Airplane       0.38      0.27      0.31      4260\n",
      "           Alarm       0.05      0.01      0.01      2180\n",
      "      Beep/Bleep       0.05      0.02      0.02      1847\n",
      "            Bell       0.39      0.06      0.11      4430\n",
      "         Bicycle       0.12      0.03      0.04       970\n",
      "      Bird Chirp       0.44      0.19      0.26     10328\n",
      "             Bus       0.08      0.02      0.03      3600\n",
      "             Car       0.28      0.16      0.20     10084\n",
      "        Cat Meow       0.20      0.02      0.04      1285\n",
      "        Chainsaw       0.72      0.12      0.21      1981\n",
      "        Clapping       0.52      0.45      0.48      2440\n",
      "           Cough       0.00      0.00      0.00        85\n",
      "         Cow Moo       0.02      0.00      0.00       651\n",
      "         Cowbell       0.62      0.14      0.23       846\n",
      "          Crying       0.42      0.11      0.17      1786\n",
      "        Dog Bark       0.64      0.21      0.31      3126\n",
      "        Doorbell       0.00      0.00      0.00       201\n",
      "            Drip       0.24      0.05      0.08      4256\n",
      "           Drums       0.27      0.09      0.14      4963\n",
      "            Fire       0.49      0.19      0.28      4052\n",
      "       Footsteps       0.18      0.10      0.13      4779\n",
      "          Guitar       0.61      0.24      0.35      7135\n",
      "          Hammer       0.02      0.00      0.00      2732\n",
      "      Helicopter       0.19      0.07      0.10      2224\n",
      "          Hiccup       0.00      0.00      0.00       513\n",
      "       Horn Honk       0.35      0.07      0.12      2482\n",
      "     Horse Neigh       0.00      0.00      0.00       138\n",
      "     Insect Buzz       0.44      0.22      0.29      4102\n",
      "      Jackhammer       0.02      0.00      0.00      1386\n",
      "        Laughter       0.28      0.03      0.06      2706\n",
      "      Lawn Mower       0.00      0.00      0.00      1142\n",
      "      Motorcycle       0.06      0.01      0.02      3710\n",
      "           Piano       0.63      0.29      0.39      6907\n",
      "        Pig Oink       0.00      0.00      0.00       327\n",
      "     Power Drill       0.04      0.01      0.02      3042\n",
      "       Power Saw       0.43      0.06      0.10      1504\n",
      "            Rain       0.24      0.27      0.25      9918\n",
      "    Rooster Crow       0.38      0.02      0.04       467\n",
      "       Saxophone       0.48      0.24      0.32      1640\n",
      "  Sewing Machine       0.01      0.00      0.00       855\n",
      "Sheep/Goat Bleat       0.52      0.08      0.14       724\n",
      "       Ship/Boat       0.12      0.03      0.05      1904\n",
      "           Shout       0.25      0.08      0.12      2323\n",
      "         Singing       0.20      0.04      0.07      4545\n",
      "           Siren       0.61      0.36      0.45      3633\n",
      "          Sneeze       0.00      0.00      0.00        15\n",
      "         Snoring       0.28      0.02      0.03      2122\n",
      "          Speech       0.29      0.09      0.13     13471\n",
      "    Stream/River       0.44      0.31      0.36      7645\n",
      "         Thunder       0.19      0.22      0.21      3327\n",
      "           Train       0.10      0.03      0.04      3423\n",
      "           Truck       0.09      0.02      0.03      5019\n",
      "         Trumpet       0.01      0.01      0.01      1076\n",
      "  Vacuum Cleaner       0.18      0.02      0.03      2066\n",
      "          Violin       0.56      0.22      0.32      2697\n",
      " Washing Machine       0.30      0.06      0.11      1988\n",
      "           Waves       0.18      0.22      0.19      4522\n",
      "            Wind       0.20      0.15      0.18     10653\n",
      "\n",
      "       micro avg       0.32      0.14      0.20    192233\n",
      "       macro avg       0.26      0.10      0.13    192233\n",
      "    weighted avg       0.31      0.14      0.18    192233\n",
      "     samples avg       0.16      0.14      0.15    192233\n",
      "\n",
      "\n",
      "Label statistics:\n",
      "True labels shape: (155057, 58)\n",
      "Predicted labels shape: (155057, 58)\n",
      "Sample true labels: [[48], [48], [48]]\n",
      "Sample predicted labels: [[10], [10], [10]]\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# FAISS IMPLEMENTATION\n",
    "# ======================\n",
    "try:\n",
    "    print(\"\\nRunning FAISS similarity search...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_vectors = scaled_train_x.astype('float32')\n",
    "    test_vectors = scaled_test_x.astype('float32')\n",
    "    \n",
    "    # Build index\n",
    "    index = faiss.IndexFlatL2(train_vectors.shape[1])\n",
    "    index.add(train_vectors)\n",
    "    \n",
    "    # Search neighbors\n",
    "    k = 5\n",
    "    D, I = index.search(test_vectors, k)\n",
    "    \n",
    "    # Aggregate predictions\n",
    "    neighbor_labels = np.array([train_y[neighbors] for neighbors in I])\n",
    "    avg_labels = neighbor_labels.mean(axis=1)\n",
    "    y_pred_faiss = (avg_labels >= 0.5).astype(int)\n",
    "    \n",
    "    # Convert to proper label format\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=range(len(categories)))\n",
    "    \n",
    "    # Convert predictions to list of lists format\n",
    "    y_pred_list = [[i for i, val in enumerate(row) if val == 1] for row in y_pred_faiss]\n",
    "    y_true_list = [[i for i, val in enumerate(row) if val == 1] for row in test_y]\n",
    "    \n",
    "    # Binarize both true and predicted labels\n",
    "    y_true_bin = mlb.fit_transform(y_true_list)\n",
    "    y_pred_bin = mlb.transform(y_pred_list)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nFAISS Classification Report:\")\n",
    "    print(classification_report(y_true_bin, y_pred_bin, \n",
    "                              target_names=categories,\n",
    "                              zero_division=0))\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(\"\\nLabel statistics:\")\n",
    "    print(f\"True labels shape: {y_true_bin.shape}\")\n",
    "    print(f\"Predicted labels shape: {y_pred_bin.shape}\")\n",
    "    print(f\"Sample true labels: {y_true_list[:3]}\")\n",
    "    print(f\"Sample predicted labels: {y_pred_list[:3]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during FAISS processing: {e}\")\n",
    "    print(f\"Test y shape: {test_y.shape if 'test_y' in locals() else 'N/A'}\")\n",
    "    print(f\"Predicted shape: {y_pred_faiss.shape if 'y_pred_faiss' in locals() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f17e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running improved FAISS similarity search...\n",
      "Applying PCA for dimensionality reduction...\n",
      "Building IVF index...\n",
      "Searching neighbors...\n",
      "Aggregating predictions...\n",
      "\n",
      "Improved FAISS Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        Airplane       0.22      0.40      0.28      4260\n",
      "           Alarm       0.01      1.00      0.03      2180\n",
      "      Beep/Bleep       0.02      0.07      0.03      1847\n",
      "            Bell       0.11      0.19      0.14      4430\n",
      "         Bicycle       0.01      1.00      0.01       970\n",
      "      Bird Chirp       0.33      0.25      0.28     10328\n",
      "             Bus       0.08      0.17      0.11      3600\n",
      "             Car       0.26      0.20      0.23     10084\n",
      "        Cat Meow       0.01      1.00      0.02      1285\n",
      "        Chainsaw       0.01      1.00      0.03      1981\n",
      "        Clapping       0.22      0.69      0.33      2440\n",
      "           Cough       0.00      1.00      0.00        85\n",
      "         Cow Moo       0.00      1.00      0.01       651\n",
      "         Cowbell       0.01      1.00      0.01       846\n",
      "          Crying       0.01      1.00      0.02      1786\n",
      "        Dog Bark       0.17      0.41      0.24      3126\n",
      "        Doorbell       0.00      1.00      0.00       201\n",
      "            Drip       0.12      0.21      0.15      4256\n",
      "           Drums       0.13      0.21      0.16      4963\n",
      "            Fire       0.21      0.41      0.28      4052\n",
      "       Footsteps       0.14      0.22      0.17      4779\n",
      "          Guitar       0.39      0.42      0.40      7135\n",
      "          Hammer       0.03      0.09      0.05      2732\n",
      "      Helicopter       0.11      0.39      0.17      2224\n",
      "          Hiccup       0.00      0.00      0.00       513\n",
      "       Horn Honk       0.09      0.27      0.13      2482\n",
      "     Horse Neigh       0.00      1.00      0.00       138\n",
      "     Insect Buzz       0.16      0.30      0.21      4102\n",
      "      Jackhammer       0.01      1.00      0.02      1386\n",
      "        Laughter       0.08      0.24      0.13      2706\n",
      "      Lawn Mower       0.01      1.00      0.01      1142\n",
      "      Motorcycle       0.07      0.15      0.10      3710\n",
      "           Piano       0.45      0.50      0.47      6907\n",
      "        Pig Oink       0.00      1.00      0.00       327\n",
      "     Power Drill       0.04      0.09      0.05      3042\n",
      "       Power Saw       0.01      1.00      0.02      1504\n",
      "            Rain       0.27      0.21      0.24      9918\n",
      "    Rooster Crow       0.00      1.00      0.01       467\n",
      "       Saxophone       0.01      1.00      0.02      1640\n",
      "  Sewing Machine       0.01      1.00      0.01       855\n",
      "Sheep/Goat Bleat       0.00      1.00      0.01       724\n",
      "       Ship/Boat       0.05      0.19      0.07      1904\n",
      "           Shout       0.07      0.24      0.11      2323\n",
      "         Singing       0.08      0.14      0.11      4545\n",
      "           Siren       0.24      0.51      0.33      3633\n",
      "          Sneeze       0.00      0.00      0.00        15\n",
      "         Snoring       0.01      1.00      0.03      2122\n",
      "          Speech       0.27      0.15      0.19     13471\n",
      "    Stream/River       0.39      0.39      0.39      7645\n",
      "         Thunder       0.15      0.35      0.21      3327\n",
      "           Train       0.07      0.15      0.09      3423\n",
      "           Truck       0.10      0.15      0.12      5019\n",
      "         Trumpet       0.01      1.00      0.01      1076\n",
      "  Vacuum Cleaner       0.01      1.00      0.03      2066\n",
      "          Violin       0.02      1.00      0.03      2697\n",
      " Washing Machine       0.01      1.00      0.03      1988\n",
      "           Waves       0.18      0.30      0.22      4522\n",
      "            Wind       0.22      0.16      0.18     10653\n",
      "\n",
      "       micro avg       0.02      0.37      0.04    192233\n",
      "       macro avg       0.10      0.55      0.12    192233\n",
      "    weighted avg       0.18      0.37      0.19    192233\n",
      "     samples avg       0.02      0.36      0.03    192233\n",
      "\n",
      "\n",
      "Hamming Loss: 0.4306\n",
      "Jaccard Score (micro): 0.0179\n",
      "Jaccard Score (macro): 0.0663\n",
      "\n",
      "Label statistics:\n",
      "True labels shape: (155057, 58)\n",
      "Predicted labels shape: (155057, 58)\n",
      "Sample true labels: [[48], [48], [48]]\n",
      "Sample predicted labels: [[1, 4, 8, 9, 10, 11, 12, 13, 14, 16, 26, 28, 30, 33, 35, 37, 38, 39, 40, 46, 52, 53, 54, 55], [1, 4, 8, 9, 10, 11, 12, 13, 14, 16, 26, 28, 30, 33, 35, 37, 38, 39, 40, 46, 52, 53, 54, 55], [1, 4, 8, 9, 10, 11, 12, 13, 14, 16, 26, 28, 30, 33, 35, 37, 38, 39, 40, 46, 48, 52, 53, 54, 55]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nKey improvements included:\\n    - PCA for dimensionality reduction\\n    - IVF index for faster searching\\n    - Cosine similarity via L2 on normalized vectors\\n    - Inverse distance weighting for neighbor votes\\n    - Dynamic thresholding per class\\n    - Additional evaluation metrics\\n    - Increased number of neighbors (k=10)\\n    - Better diagnostic information\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================\n",
    "# IMPROVED FAISS IMPLEMENTATION\n",
    "# ======================\n",
    "try:\n",
    "    print(\"\\nRunning improved FAISS similarity search...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_vectors = scaled_train_x.astype('float32')\n",
    "    test_vectors = scaled_test_x.astype('float32')\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    print(\"Applying PCA for dimensionality reduction...\")\n",
    "    pca_dim = min(64, train_vectors.shape[1])  # Reduce to max 64 dimensions\n",
    "    pca = faiss.PCAMatrix(train_vectors.shape[1], pca_dim)\n",
    "    pca.train(train_vectors)\n",
    "    train_vectors = pca.apply(train_vectors)\n",
    "    test_vectors = pca.apply(test_vectors)\n",
    "    \n",
    "    # Build improved index with IVF\n",
    "    print(\"Building IVF index...\")\n",
    "    nlist = 100  # Number of clusters\n",
    "    quantizer = faiss.IndexFlatL2(pca_dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, pca_dim, nlist)\n",
    "    \n",
    "    # Train and add vectors\n",
    "    index.train(train_vectors)\n",
    "    index.add(train_vectors)\n",
    "    index.nprobe = 10  # Number of clusters to search\n",
    "    \n",
    "    # Search neighbors with cosine similarity\n",
    "    print(\"Searching neighbors...\")\n",
    "    k = 10  # Increased number of neighbors\n",
    "    D, I = index.search(test_vectors, k)\n",
    "    \n",
    "    # Weighted aggregation by inverse distance\n",
    "    print(\"Aggregating predictions...\")\n",
    "    neighbor_labels = np.array([train_y[neighbors] for neighbors in I])\n",
    "    weights = 1.0 / (D + 1e-6)  # Inverse distance weighting\n",
    "    weighted_labels = np.sum(neighbor_labels * weights[:, :, np.newaxis], axis=1)\n",
    "    weighted_labels /= np.sum(weights, axis=1)[:, np.newaxis]  # Normalize\n",
    "    \n",
    "    # Dynamic thresholding per class\n",
    "    y_pred_faiss = np.zeros_like(weighted_labels)\n",
    "    for i in range(weighted_labels.shape[1]):\n",
    "        class_values = weighted_labels[:, i]\n",
    "        threshold = np.percentile(class_values, 95) if np.max(class_values) > 0.5 else 0.5\n",
    "        y_pred_faiss[:, i] = (class_values >= threshold).astype(int)\n",
    "    \n",
    "    # Convert to proper label format\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=range(len(categories)))\n",
    "    \n",
    "    # Convert predictions to list of lists format\n",
    "    y_pred_list = [[i for i, val in enumerate(row) if val == 1] for row in y_pred_faiss]\n",
    "    y_true_list = [[i for i, val in enumerate(row) if val == 1] for row in test_y]\n",
    "    \n",
    "    # Binarize both true and predicted labels\n",
    "    y_true_bin = mlb.fit_transform(y_true_list)\n",
    "    y_pred_bin = mlb.transform(y_pred_list)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nImproved FAISS Classification Report:\")\n",
    "    print(classification_report(y_true_bin, y_pred_bin, \n",
    "                              target_names=categories,\n",
    "                              zero_division=0))\n",
    "    \n",
    "    # Additional metrics\n",
    "    from sklearn.metrics import hamming_loss, jaccard_score\n",
    "    print(f\"\\nHamming Loss: {hamming_loss(y_true_bin, y_pred_bin):.4f}\")\n",
    "    print(f\"Jaccard Score (micro): {jaccard_score(y_true_bin, y_pred_bin, average='micro'):.4f}\")\n",
    "    print(f\"Jaccard Score (macro): {jaccard_score(y_true_bin, y_pred_bin, average='macro'):.4f}\")\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(\"\\nLabel statistics:\")\n",
    "    print(f\"True labels shape: {y_true_bin.shape}\")\n",
    "    print(f\"Predicted labels shape: {y_pred_bin.shape}\")\n",
    "    print(f\"Sample true labels: {y_true_list[:3]}\")\n",
    "    print(f\"Sample predicted labels: {y_pred_list[:3]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during improved FAISS processing: {e}\")\n",
    "    print(f\"Test y shape: {test_y.shape if 'test_y' in locals() else 'N/A'}\")\n",
    "    print(f\"Predicted shape: {y_pred_faiss.shape if 'y_pred_faiss' in locals() else 'N/A'}\")\n",
    "\n",
    "\"\"\"\n",
    "Key improvements included:\n",
    "    - PCA for dimensionality reduction\n",
    "    - IVF index for faster searching\n",
    "    - Cosine similarity via L2 on normalized vectors\n",
    "    - Inverse distance weighting for neighbor votes\n",
    "    - Dynamic thresholding per class\n",
    "    - Additional evaluation metrics\n",
    "    - Increased number of neighbors (k=10)\n",
    "    - Better diagnostic information\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a134ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running optimized FAISS similarity search...\n",
      "Applying PCA for dimensionality reduction...\n",
      "Building IVF index...\n",
      "Training index...\n",
      "Adding vectors to index...\n",
      "Searching 10 neighbors...\n",
      "Aggregating predictions with optimized weighting...\n",
      "Applying dynamic thresholds...\n",
      "Applying intelligent post-processing...\n",
      "\n",
      "Optimized FAISS Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        Airplane       0.22      0.35      0.27      4260\n",
      "           Alarm       0.07      0.02      0.03      2180\n",
      "      Beep/Bleep       0.03      0.02      0.02      1847\n",
      "            Bell       0.26      0.09      0.13      4430\n",
      "         Bicycle       0.04      0.03      0.03       970\n",
      "      Bird Chirp       0.29      0.27      0.28     10328\n",
      "             Bus       0.08      0.05      0.06      3600\n",
      "             Car       0.22      0.30      0.26     10084\n",
      "        Cat Meow       0.09      0.05      0.06      1285\n",
      "        Chainsaw       0.49      0.16      0.24      1981\n",
      "        Clapping       0.38      0.57      0.45      2440\n",
      "           Cough       0.00      0.00      0.00        85\n",
      "         Cow Moo       0.04      0.01      0.02       651\n",
      "         Cowbell       0.41      0.17      0.24       846\n",
      "          Crying       0.21      0.15      0.18      1786\n",
      "        Dog Bark       0.38      0.30      0.34      3126\n",
      "        Doorbell       0.10      0.06      0.08       201\n",
      "            Drip       0.19      0.12      0.15      4256\n",
      "           Drums       0.22      0.13      0.16      4963\n",
      "            Fire       0.30      0.32      0.31      4052\n",
      "       Footsteps       0.12      0.27      0.17      4779\n",
      "          Guitar       0.50      0.33      0.40      7135\n",
      "          Hammer       0.04      0.02      0.02      2732\n",
      "      Helicopter       0.18      0.18      0.18      2224\n",
      "          Hiccup       0.00      0.00      0.00       513\n",
      "       Horn Honk       0.25      0.12      0.16      2482\n",
      "     Horse Neigh       0.00      0.00      0.00       138\n",
      "     Insect Buzz       0.29      0.25      0.27      4102\n",
      "      Jackhammer       0.01      0.00      0.01      1386\n",
      "        Laughter       0.16      0.07      0.09      2706\n",
      "      Lawn Mower       0.00      0.00      0.00      1142\n",
      "      Motorcycle       0.07      0.05      0.06      3710\n",
      "           Piano       0.55      0.41      0.47      6907\n",
      "        Pig Oink       0.00      0.00      0.00       327\n",
      "     Power Drill       0.04      0.02      0.03      3042\n",
      "       Power Saw       0.29      0.09      0.14      1504\n",
      "            Rain       0.21      0.35      0.26      9918\n",
      "    Rooster Crow       0.28      0.04      0.08       467\n",
      "       Saxophone       0.37      0.32      0.35      1640\n",
      "  Sewing Machine       0.02      0.01      0.01       855\n",
      "Sheep/Goat Bleat       0.27      0.12      0.17       724\n",
      "       Ship/Boat       0.10      0.07      0.08      1904\n",
      "           Shout       0.15      0.12      0.13      2323\n",
      "         Singing       0.13      0.08      0.10      4545\n",
      "           Siren       0.48      0.43      0.45      3633\n",
      "          Sneeze       0.00      0.00      0.00        15\n",
      "         Snoring       0.10      0.03      0.05      2122\n",
      "          Speech       0.24      0.22      0.23     13471\n",
      "    Stream/River       0.34      0.44      0.38      7645\n",
      "         Thunder       0.14      0.37      0.21      3327\n",
      "           Train       0.08      0.07      0.07      3423\n",
      "           Truck       0.08      0.04      0.06      5019\n",
      "         Trumpet       0.03      0.02      0.02      1076\n",
      "  Vacuum Cleaner       0.14      0.05      0.08      2066\n",
      "          Violin       0.43      0.29      0.34      2697\n",
      " Washing Machine       0.20      0.12      0.15      1988\n",
      "           Waves       0.16      0.33      0.21      4522\n",
      "            Wind       0.17      0.20      0.18     10653\n",
      "\n",
      "       micro avg       0.23      0.22      0.22    192233\n",
      "       macro avg       0.18      0.15      0.15    192233\n",
      "    weighted avg       0.23      0.22      0.21    192233\n",
      "     samples avg       0.23      0.22      0.21    192233\n",
      "\n",
      "\n",
      "Key Metrics:\n",
      "Micro F1: 0.222\n",
      "Macro F1: 0.154\n",
      "Hamming Loss: 0.0325\n",
      "Jaccard Score (micro): 0.1249\n",
      "Jaccard Score (macro): 0.0891\n",
      "\n",
      "Key Category Performance:\n",
      "Clapping        F1: 0.454 (P: 0.024, R: 0.016)\n",
      "Guitar          F1: 0.398 (P: 0.030, R: 0.046)\n",
      "Piano           F1: 0.470 (P: 0.033, R: 0.045)\n",
      "Siren           F1: 0.453 (P: 0.021, R: 0.023)\n",
      "Dog Bark        F1: 0.335 (P: 0.016, R: 0.020)\n",
      "Rain            F1: 0.262 (P: 0.109, R: 0.064)\n",
      "Chainsaw        F1: 0.245 (P: 0.004, R: 0.013)\n",
      "Violin          F1: 0.342 (P: 0.012, R: 0.017)\n",
      "Speech          F1: 0.232 (P: 0.079, R: 0.087)\n",
      "Stream/River    F1: 0.382 (P: 0.064, R: 0.049)\n",
      "\n",
      "Confidence and Support Analysis:\n",
      "Clapping       :  2440 support | Conf: 0.57 | Pred: 3647.0\n",
      "Guitar         :  7135 support | Conf: 0.59 | Pred: 4717.0\n",
      "Piano          :  6907 support | Conf: 0.57 | Pred: 5144.0\n",
      "Siren          :  3633 support | Conf: 0.60 | Pred: 3255.0\n",
      "Dog Bark       :  3126 support | Conf: 0.43 | Pred: 2468.0\n",
      "Rain           :  9918 support | Conf: 0.51 | Pred: 16930.0\n",
      "Chainsaw       :  1981 support | Conf: 0.54 | Pred: 667.0\n",
      "Violin         :  2697 support | Conf: 0.54 | Pred: 1802.0\n",
      "Speech         : 13471 support | Conf: 0.40 | Pred: 12202.0\n",
      "Stream/River   :  7645 support | Conf: 0.54 | Pred: 10001.0\n",
      "Cough          :    85 support | Conf: 0.16 | Pred: 129.0\n",
      "Sneeze         :    15 support | Conf: 0.12 | Pred:  14.0\n",
      "Horse Neigh    :   138 support | Conf: 0.26 | Pred:  78.0\n",
      "\n",
      "Prediction Statistics:\n",
      "True labels shape: (155057, 58)\n",
      "Avg labels/sample - True: 1.24\n",
      "Avg labels/sample - Pred: 1.18\n",
      "Samples adjusted (overlimit): 4786\n",
      "Samples adjusted (empty): 0\n",
      "\n",
      "Sample predictions (first 3):\n",
      "Sample 0: ['Clapping']\n",
      "Sample 1: ['Clapping']\n",
      "Sample 2: ['Clapping', 'Stream/River']\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# COMPLETE OPTIMIZED FAISS IMPLEMENTATION\n",
    "# ======================\n",
    "try:\n",
    "    print(\"\\nRunning optimized FAISS similarity search...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_vectors = scaled_train_x.astype('float32')\n",
    "    test_vectors = scaled_test_x.astype('float32')\n",
    "    \n",
    "    # 1. Improved PCA\n",
    "    print(\"Applying PCA for dimensionality reduction...\")\n",
    "    pca_dim = min(32, train_vectors.shape[1])\n",
    "    pca = faiss.PCAMatrix(train_vectors.shape[1], pca_dim)\n",
    "    pca.train(train_vectors)\n",
    "    train_vectors = pca.apply(train_vectors)\n",
    "    test_vectors = pca.apply(test_vectors)\n",
    "    \n",
    "    # 2. Improved IVF Index\n",
    "    print(\"Building IVF index...\")\n",
    "    nlist = 200\n",
    "    quantizer = faiss.IndexFlatL2(pca_dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, pca_dim, nlist)\n",
    "    \n",
    "    if not index.is_trained:\n",
    "        print(\"Training index...\")\n",
    "        index.train(train_vectors)\n",
    "    \n",
    "    print(\"Adding vectors to index...\")\n",
    "    index.add(train_vectors)\n",
    "    index.nprobe = 20\n",
    "    \n",
    "    # Search neighbors\n",
    "    k = 10\n",
    "    print(f\"Searching {k} neighbors...\")\n",
    "    D, I = index.search(test_vectors, k)\n",
    "    \n",
    "    # ======================\n",
    "    # ENHANCED NEIGHBOR AGGREGATION\n",
    "    # ======================\n",
    "    print(\"Aggregating predictions with optimized weighting...\")\n",
    "    \n",
    "    # Get neighbor labels in correct 3D shape [n_samples, k, n_classes]\n",
    "    neighbor_labels = np.stack([train_y[neighbors] for neighbors in I])\n",
    "    \n",
    "    # Calculate class frequencies and weights\n",
    "    class_freq = np.mean(train_y, axis=0)\n",
    "    class_weights = 1 / (class_freq + 0.1)  # Inverse frequency weighting\n",
    "    \n",
    "    # Enhanced weighting scheme\n",
    "    D_normalized = D / (np.max(D) + 1e-6)\n",
    "    weights = (\n",
    "        np.exp(-2 * D_normalized)[:, :, np.newaxis] *  # Stronger distance decay\n",
    "        (class_weights[np.newaxis, np.newaxis, :] *    # Class balancing\n",
    "         (1 + np.mean(neighbor_labels, axis=1)[:, np.newaxis, :])  # Local prevalence\n",
    "    ))\n",
    "    \n",
    "    # Weighted aggregation\n",
    "    weighted_labels = np.sum(neighbor_labels * weights, axis=1)\n",
    "    weighted_labels /= (np.sum(weights, axis=1) + 1e-6)\n",
    "    \n",
    "    # ======================\n",
    "    # DYNAMIC THRESHOLDING\n",
    "    # ======================\n",
    "    print(\"Applying dynamic thresholds...\")\n",
    "    \n",
    "    y_pred_faiss = np.zeros_like(weighted_labels)\n",
    "    for i, category in enumerate(categories):\n",
    "        # Base threshold adjusts based on class frequency\n",
    "        base_thresh = 0.7 - (0.4 * class_freq[i])  # More common -> lower threshold\n",
    "        \n",
    "        # Adjust by confidence distribution\n",
    "        conf_adj = np.percentile(weighted_labels[:, i], 70) / 0.7\n",
    "        threshold = max(0.3, min(0.8, base_thresh * conf_adj))\n",
    "        \n",
    "        # Special cases\n",
    "        if category in ['Rain', 'Waves', 'Wind']:\n",
    "            threshold = max(threshold, 0.6)  # Higher threshold for high-recall classes\n",
    "        elif category in ['Cough', 'Sneeze', 'Horse Neigh']:\n",
    "            threshold = 0.8  # Very high threshold for rare classes\n",
    "            \n",
    "        y_pred_faiss[:, i] = (weighted_labels[:, i] >= threshold).astype(int)\n",
    "    \n",
    "    # ======================\n",
    "    # INTELLIGENT POST-PROCESSING\n",
    "    # ======================\n",
    "    print(\"Applying intelligent post-processing...\")\n",
    "    \n",
    "    avg_true_labels = np.mean([len(x) for x in y_true_list]) if 'y_true_list' in locals() else 1.24\n",
    "    max_labels_per_sample = min(5, int(avg_true_labels * 2))\n",
    "    \n",
    "    high_recall_classes = [categories.index(c) for c in ['Rain', 'Waves', 'Wind']]\n",
    "    rare_classes = [categories.index(c) for c in ['Cough', 'Sneeze', 'Horse Neigh', 'Pig Oink']]\n",
    "    \n",
    "    for i in range(y_pred_faiss.shape[0]):\n",
    "        preds = y_pred_faiss[i]\n",
    "        conf_scores = weighted_labels[i]\n",
    "        \n",
    "        # Special handling for high-recall classes\n",
    "        preds[high_recall_classes] = (conf_scores[high_recall_classes] > 0.6).astype(int)\n",
    "        \n",
    "        # Special handling for rare classes\n",
    "        preds[rare_classes] = 0\n",
    "        for rc in rare_classes:\n",
    "            if conf_scores[rc] > 0.8:\n",
    "                preds[rc] = 1\n",
    "                \n",
    "        # Global constraint\n",
    "        if preds.sum() > max_labels_per_sample:\n",
    "            top_indices = np.argpartition(conf_scores, -3)[-3:]  # Keep top 3\n",
    "            preds[:] = 0\n",
    "            preds[top_indices] = 1\n",
    "        elif preds.sum() == 0:\n",
    "            preds[np.argmax(conf_scores)] = 1  # Ensure at least one prediction\n",
    "\n",
    "    # ======================\n",
    "    # COMPREHENSIVE EVALUATION\n",
    "    # ======================\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    from sklearn.metrics import classification_report, f1_score, hamming_loss, jaccard_score\n",
    "    \n",
    "    # Prepare labels\n",
    "    if 'y_true_list' not in locals():\n",
    "        y_true_list = [[i for i, val in enumerate(row) if val == 1] for row in test_y]\n",
    "    y_pred_list = [[i for i, val in enumerate(row) if val == 1] for row in y_pred_faiss]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer(classes=range(len(categories)))\n",
    "    y_true_bin = mlb.fit_transform(y_true_list)\n",
    "    y_pred_bin = mlb.transform(y_pred_list)\n",
    "    \n",
    "    # Main classification report\n",
    "    print(\"\\nOptimized FAISS Classification Report:\")\n",
    "    print(classification_report(y_true_bin, y_pred_bin, \n",
    "                              target_names=categories,\n",
    "                              zero_division=0))\n",
    "    \n",
    "    # Key metrics\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(f\"Micro F1: {f1_score(y_true_bin, y_pred_bin, average='micro'):.3f}\")\n",
    "    print(f\"Macro F1: {f1_score(y_true_bin, y_pred_bin, average='macro'):.3f}\")\n",
    "    print(f\"Hamming Loss: {hamming_loss(y_true_bin, y_pred_bin):.4f}\")\n",
    "    print(f\"Jaccard Score (micro): {jaccard_score(y_true_bin, y_pred_bin, average='micro'):.4f}\")\n",
    "    print(f\"Jaccard Score (macro): {jaccard_score(y_true_bin, y_pred_bin, average='macro'):.4f}\")\n",
    "    \n",
    "    # Focused category analysis\n",
    "    print(\"\\nKey Category Performance:\")\n",
    "    key_categories = ['Clapping', 'Guitar', 'Piano', 'Siren', 'Dog Bark', \n",
    "                     'Rain', 'Chainsaw', 'Violin', 'Speech', 'Stream/River']\n",
    "    for cat in key_categories:\n",
    "        idx = categories.index(cat)\n",
    "        print(f\"{cat.ljust(15)} F1: {f1_score(y_true_bin[:,idx], y_pred_bin[:,idx]):.3f} \"\n",
    "              f\"(P: {y_pred_bin[:,idx].mean():.3f}, R: {y_true_bin[:,idx].mean():.3f})\")\n",
    "    \n",
    "    # Confidence and support analysis\n",
    "    print(\"\\nConfidence and Support Analysis:\")\n",
    "    analysis_categories = key_categories + ['Cough', 'Sneeze', 'Horse Neigh']\n",
    "    for cat in analysis_categories:\n",
    "        idx = categories.index(cat)\n",
    "        if y_pred_faiss[:,idx].sum() > 0:\n",
    "            conf = weighted_labels[y_pred_faiss[:,idx].astype(bool),idx].mean()\n",
    "            support = y_true_bin[:,idx].sum()\n",
    "            print(f\"{cat.ljust(15)}: {support:>5} support | \"\n",
    "                  f\"Conf: {conf:.2f} | \"\n",
    "                  f\"Pred: {y_pred_faiss[:,idx].sum():>5}\")\n",
    "    \n",
    "    # Prediction statistics\n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(f\"True labels shape: {y_true_bin.shape}\")\n",
    "    print(f\"Avg labels/sample - True: {y_true_bin.sum(axis=1).mean():.2f}\")\n",
    "    print(f\"Avg labels/sample - Pred: {y_pred_bin.sum(axis=1).mean():.2f}\")\n",
    "    print(f\"Samples adjusted (overlimit): {(y_pred_faiss.sum(axis=1) > max_labels_per_sample).sum()}\")\n",
    "    print(f\"Samples adjusted (empty): {(y_pred_faiss.sum(axis=1) == 0).sum()}\")\n",
    "    print(f\"\\nSample predictions (first 3):\")\n",
    "    for i in range(3):\n",
    "        pred_labels = [categories[idx] for idx in y_pred_list[i]]\n",
    "        print(f\"Sample {i}: {pred_labels}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during FAISS processing: {e}\")\n",
    "    print(f\"Test y shape: {test_y.shape if 'test_y' in locals() else 'N/A'}\")\n",
    "    print(f\"Predicted shape: {y_pred_faiss.shape if 'y_pred_faiss' in locals() else 'N/A'}\")\n",
    "    print(f\"PCA dimensions: {pca_dim if 'pca_dim' in locals() else 'N/A'}\")\n",
    "    print(f\"Index type: {type(index).__name__ if 'index' in locals() else 'N/A'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
